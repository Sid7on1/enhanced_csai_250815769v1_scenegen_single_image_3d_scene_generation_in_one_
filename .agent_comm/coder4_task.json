{
  "agent_id": "coder4",
  "task_id": "task_2",
  "files": [
    {
      "name": "requirements.txt",
      "purpose": "Python dependencies",
      "priority": "high"
    },
    {
      "name": "dataset_downloader.py",
      "purpose": "HuggingFace dataset integration",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.AI_2508.15769v1_SceneGen_Single_Image_3D_Scene_Generation_in_One_",
    "project_type": "transformer",
    "description": "Enhanced AI project based on cs.AI_2508.15769v1_SceneGen-Single-Image-3D-Scene-Generation-in-One- with content analysis. Detected project type: transformer (confidence score: 7 matches).",
    "key_algorithms": [
      "Scenegen",
      "Bottom-Up",
      "Cloud",
      "Machine",
      "Responding",
      "Image",
      "Generative",
      "Mesh",
      "Mri",
      "Ception"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.AI_2508.15769v1_SceneGen-Single-Image-3D-Scene-Generation-in-One-.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nSceneGen: Single-Image 3D Scene Generation in One Feedforward Pass\nYanxu Meng\u2217, Haoning Wu\u2217, Ya Zhang, Weidi Xie\u2020\nSchool of Artificial Intelligence, Shanghai Jiao Tong University\nFigure 1. Overview. Our proposed SceneGen framework takes a single scene image and its corresponding object masks as inputs, and\nefficiently generates multiple 3D assets with coherent geometry, texture, and spatial arrangement in a single feedforward pass.\nAbstract\n3D content generation has recently attracted significant\nresearch interest due to its applications in VR/AR and em-\nbodied AI. In this work, we address the challenging task\nofsynthesizing multiple 3D assets within a single scene\nimage . Concretely, our contributions are fourfold: (i) we\npresent SceneGen, a novel framework that takes a scene\nimage and corresponding object masks as input, simulta-\nneously producing multiple 3D assets with geometry and\ntexture. Notably, SceneGen operates with no need for opti-\nmization or asset retrieval; (ii) we introduce a novel feature\naggregation module that integrates local and global scene\ninformation from visual and geometric encoders within the\n*: These authors contribute equally to this work.\n\u2020: Corresponding author.feature extraction module. Coupled with a position head ,\nthis enables the generation of 3D assets and their rela-\ntive spatial positions in a single feedforward pass; (iii) we\ndemonstrate SceneGen\u2019s direct extensibility to multi-image\ninput scenarios. Despite being trained solely on single-\nimage inputs, our architectural design enables improved\ngeneration performance with multi-image inputs; and (iv)\nextensive quantitative and qualitative evaluations confirm\nthe efficiency and robust generation abilities of our ap-\nproach. We believe this paradigm offers a novel solution\nfor high-quality 3D content generation, potentially advanc-\ning its practical applications in downstream tasks. The\ncode and model will be publicly available at: https:\n//mengmouxu.github.io/SceneGen .\n\u201cEverything you can imagine is real. \u201d\n\u2014\u2014 Pablo Picasso\n1arXiv:2508.15769v1  [cs.CV]  21 Aug 2025\n\n--- Page 2 ---\n1. Introduction\nThe growing demand for immersive digital environments\nin applications such as virtual/augmented reality (VR/AR)\nand embodied AI has spurred significant advancements in\n3D content generation [6, 8, 9, 13, 14, 28]. While early\nefforts primarily focused on synthesizing individual 3D as-\nsets [33, 59, 69], recent research focus has shifted to the\nmore challenging task of 3D scene generation. Generat-\ning realistic 3D scenes [5, 11, 15, 16, 19, 64, 65], condi-\ntioned on input text or images, requires synthesizing mul-\ntiple assets with accurate geometry, texture, and spatial re-\nlationships. This primarily hinges on two key capabilities:\n(i)3D asset generation for generating plausible asset geo-\nmetric topologies from limited textual or visual input. (ii)\nspatial arrangement for managing inter-object spatial re-\nlationships to correctly handle support, occlusion, and other\nphysical interactions among assets.\nExisting approaches can be divided into two paradigms:\n(i) retrieval-based methods [12, 38, 49, 61] typically employ\nLLMs for layout planning and retrieve matching 3D assets\nfrom existing libraries to assemble scenes. While straight-\nforward, their flexibility is limited by the coverage of avail-\nable assets. (ii) two-stage approaches [18, 35, 62] first gen-\nerate individual 3D assets, then employ Vision-Language\nModels (VLMs) or optimization techniques to refine scene\nstructure and spatial arrangement. Although more flexi-\nble, their reliance on iterative optimization inevitably leads\nto inefficiency and error accumulation. The most relevant\nworks to ours are MIDI [23] and PartCrafter [34], which\ngenerate multiple assets or parts from a single image. How-\never, they still suffer from limited synthesis fidelity and in-\naccurate spatial relationships among assets.\nTo address the aforementioned challenges, we propose\nSceneGen , a novel 3D scene generation model designed to\nsimultaneously generate multiple assets, including their ge-\nometry, texture, and spatial positions, within a scene image,\nall in a single feedforward pass. Concretely, our framework\nbuilds upon an existing single-asset generation model [59]\nand introduces three key modules, including feature ex-\ntraction ,feature aggregation , and output module. First,\nthe feature extraction module strategically leverages off-\nthe-shelf visual [42] and geometric [53] encoders to extract\nboth asset-level and scene-level features within the scene.\nSubsequently, our proposed feature aggregation module,\ncomposed of local and global attention blocks, effectively\nintegrates the extracted visual and geometric features while\nenabling inter-asset interactions during generation to ensure\nplausible geometric topologies. Benefiting from thorough\nscene information utilization, the latent features generated\nby SceneGen can be directly decoded into assets\u2019 relative\nposition, geometry, and texture through the position head\nand pretrained structure decoder within our output module.\nMoreover, despite being trained solely on single-imagesamples, SceneGen demonstrates remarkable generaliza-\ntion capability to multi-image input scenarios, achieving\neven better generation quality, which primarily stems from\nour dedicated architectural design. To conduct a compre-\nhensive and reliable evaluation of SceneGen\u2019s performance\non 3D scene generation, we systematically adopt multiple\nmetrics focusing on both geometric and visual quality. Both\nquantitative and qualitative results demonstrate that our pro-\nposed SceneGen significantly outperforms previous meth-\nods in terms of generation quality and efficiency, which can\ngenerate textured scenes with four assets in about 2 minutes.\nThe rest of this paper is organized as follows: In Sec. 2,\nwe provide a comprehensive review and discussion of re-\nlated literature; Sec. 3 elaborates on details of our proposed\nSceneGen; Sec. 4 presents extensive quantitative and qual-\nitative evaluations; Finally, Sec. 5 concludes with key in-\nsights and contributions of our work. To our knowledge,\nSceneGen is the first 3D scene generation model capable\nof simultaneously synthesizing geometry, texture, and rela-\ntive positions of multiple 3D assets in a single feedforward\npass, without requiring optimization. We believe this work\nwill inspire advances in high-quality, efficient 3D content\ngeneration and facilitate applications in downstream tasks.\n2. Related Work\n3D visual perception. Extensive research has advanced 3D\nvisual perception, where traditional methods like SfM [48,\n52] rely on computationally intensive optimization for\n3D reconstruction. Notably, emerging feedforward meth-\nods [3, 27, 30, 51, 53\u201355, 68, 71] have demonstrated ef-\nficient 3D perception, with DUSt3R [55] pioneering this\ntrend and VGGT [53] establishing a minimalist yet power-\nful paradigm that distills geometric priors from large-scale\ndata without explicit 3D inductive biases or optimizations.\n3D asset synthesis. Typically, 3D asset synthesis aims to\ngenerate object-centric geometry and texture from text or\nimage inputs. The recent success of diffusion models [22] in\n2D generation [37, 43, 47, 56, 57] has inspired the develop-\nment of learning-based, scalable 3D content [6, 8, 9, 13, 14,\n28] generation, which produce 3D asset in various represen-\ntations, including explicit forms such as point clouds [40],\nvoxels [25, 41], and SDFs [4, 32], as well as implicit ones\nlike 3D Gaussians [20, 67] and NeRFs [1, 31, 60]. Sub-\nsequent advances leverage V AEs [29] for compressing 3D\ngeometry or textures [33, 59, 69] and adopt hybrid mesh-\ntexture pipelines [24, 26, 58, 66], with TRELLIS [59]\ndemonstrating scalable, high-fidelity generation via struc-\ntured latents. Nevertheless, these methods remain restricted\nto single-asset synthesis and fundamentally lack the ability\nto model complex multi-asset scenes.\n3D scene generation. Beyond single asset synthesis, 3D\nscene generation is more challenging yet valuable, aim-\n2\n\n--- Page 3 ---\nObject N\nVLMPre-process Modules3D Instance GenerationPost-optimization StepsObject 1\u2026Multi-Instance 3D Asset GenerationPos. TokensMulti-Instance StructureGeneration ModuleAssets Latent\nDecoderPosition HeadScene CompositionTextured Assets\n(a) Scene image segmentationFlorence-2Depth Model\u20263D Instance RetrievalPoint Cloud AlignmentVLMScene GraphRenderProjectionPhysical Constrain\u2026(b) Two-stageapproaches(c) Single-image to 3DsceneRelative Positions\n(d) Our SceneGen paradigm\nFigure 2. 3D Scene Generation. (a) Existing methods typically\nrequire segmenting target objects from the scene image; (b) Two-\nstage methods like CAST [62] sequentially retrieve or generate in-\ndividual assets, then assemble them via post-processing; (c) Meth-\nods such as MIDI [23] directly generate multiple assets from a\nsingle image, but suffer from blurry details and unreasonable spa-\ntial layouts; (d) In contrast, our SceneGen jointly synthesizes the\ngeometry, texture, and spatial positions of multiple assets in a sin-\ngle feedforward pass, producing plausible 3D scenes.\ning to produce multiple coordinated, physically plausi-\nble assets within a scene. Prior text-based approaches\nprimarily leverage LLMs for layout planning [12, 38,\n49, 61] and retrieve suitable assets from existing li-\nbraries. Subsequent image-based methods employ segmen-\ntation [5, 10, 15, 19, 64], scene graph [11, 16, 65] and\ndepth/point cloud alignment [10, 50, 62] to assist multi-\nasset generation and arrangement. As depicted in Fig-\nure 2 (b), recent optimization-based methods [18, 35, 62]\nadopt VLMs for post-processing, refining scene structures\nthrough image/text-guided adjustments, but inevitably suf-\nfer from inefficiency. Other works (Figure 2 (c)), such as\nMIDI [23] and PartCrafter [34], explore scene generation\nconditioned on a single image, but inherently sacrifice re-\nconstruction fidelity due to their canonical-space represen-\ntations. Our proposed SceneGen uniquely overcomes these\nlimitations by integrating asset-level and scene-level fea-\ntures, enabling robust and efficient 3D scene generation.\n3. Method\nIn this work, we present SceneGen , designed to jointly per-\nform 3D asset generation within scenes and predict rela-\ntive spatial positions among assets. Here, we first formally\ndescribe our problem formulation in Sec. 3.1; followed by\nelaboration on our model architecture and training method-\nology in Sec. 3.2 and 3.3, respectively; finally, we extendSceneGen to multi-view input scenarios in Sec. 3.4.\n3.1. Problem Formulation\nOur proposed SceneGen is a single-stage feedforward 3D\nscene generation model ( GScene ), which takes a scene\nimage ( IScene ) containing Nobjects and corresponding\nmasks ( {mi}N\ni=1) as input, simultaneously generating 3D\nasset structure and texture representations ( {Si}N\ni=1), and\ntheir relative positions ( {Pi}N\ni=1), formulated as:\n{(Si,Pi)}N\ni=1=GScene({mi}N\ni=1,IScene)\nHere, the position of each asset relative to a pre-selected\nquery asset, is denoted as Pi= [ti,qi,si]\u2208R8, compris-\ningti\u2208R3(translation), qi\u2208R4(rotation quaternion),\nandsi\u2208R1(scale factor). By default, we select the asset\nwithi= 1 as the query asset, with its parameters fixed as:\ntquery = [0,0,0],qquery = [1,0,0,0],squery = 1.\n3.2. SceneGen\nOur proposed SceneGen ( GScene ) comprises three key\nstages: (i) feature extraction , employing a scene visual en-\ncoder ( \u03a6V) and a scene geometric encoder ( \u03a6G) to extract\nvisual and structural features within the scene, implemented\nusing off-the-shelf DINOv2 [42] and VGGT [53], respec-\ntively; (ii) feature aggregation , comprising MDiT [43]\nblocks, each integrating a local attention block, a global\nattention block, and a feedforward network; and (iii) out-\nputmodule, which introduces a position head ( \u03a8pos) for\npredicting the spatial locations of assets and adopts off-the-\nshelf sparse structure (SS) and structured latents (SLAT)\ndecoders [59] for decoding scene geometry structures. By\nintegrating these complementary modules, our SceneGen\neffectively captures both local asset-level and global scene-\nlevel features, enabling it to simultaneously generate multi-\nple 3D assets and predict their relative positions.\nFeature extraction. Our SceneGen starts with extract-\ning both local and global features from a given scene im-\nage (Iscene ) with the visual encoder ( \u03a6V) and geometric\nencoder ( \u03a6G). Specifically, for each object with its cor-\nresponding segmentation mask ( mi), we obtain four com-\nplementary feature representations: (i) the object\u2019s indi-\nvidual visual features ( FV\ni); (ii) the visual features of its\nmask ( Fmask\ni ); (iii) scene global visual features ( FV\nglobal );\nand (iv) the global geometric features ( Fgeo\nglobal), denoted as:\nFV\ni= \u03a6 V(Iscene\u2297mi),Fmask\ni = \u03a6 V(mi),\nFV\nglobal = \u03a6 V(Iscene),Fgeo\nglobal= \u03a6 G(Iscene)\nHere,\u2297represents pixel-wise multiplication. These fea-\ntures are then concatenated along the sequence dimension\ninto a unified scene context ( Fscene\ni ), formulated as:\nFscene\ni = [FV\ni;Fmask\ni;FV\nglobal ;Fgeo\nglobal]\n3\n\n--- Page 4 ---\nLocal AttentionGlobal Attention\ud835\udc74\u00d7DiT BlocksPatchifyLinear\nLinearUnpatchifyPositionEmbed.AssetSelf-Attn.(AS)AssetCross-Attn.(AC)SceneSelf-Attn.(SS)SceneCross-Attn.(SC)FFNGeometricEncoder\u03a6!Timestep EmbeddingPos. Tokens\nPositionHead\u03a8\"#$Query Pos.\nnoisy sparse structure latents\t\ud835\udc31%%&'(\nsegmented objectsVisualEncoder\u03a6)\n\u2131*+#,-+*.#\u2131%)Pos. Tokens\u2131%$/.0.\nAsset 4\ud835\udc95\nAsset 4 Pos.Query AssetAsset 3\nAsset 2 Pos.Asset 3 Pos.Asset 2\nscene image \ud835\udc70$/.0.SSDecoderSLATDecoder\n: concatenate\n: frozenparameters: trainableparameters\u2131%1-$2\u2131*+#,-+)\nobject masks \ud835\udc8e%%&'(\n3D scene containing multiple 3D assets: positionencodingConditions\nScene\n(ii) Feature Aggregation(i) Feature Extraction(iii) Output ModuleFigure 3. Architecture Overview. SceneGen takes a single scene image with multiple objects and corresponding segmentation masks\nas input. A pre-trained local attention block first refines the texture of each asset. Then, our introduced global attention block integrates\nasset-level and scene-level features extracted by dedicated visual and geometric encoders. Finally, two off-the-shelf structure decoders and\nourposition head decode these latent features into multiple 3D assets with geometry, texture, and relative spatial positions.\nFeature aggregation. Subsequently, SceneGen employs a\nfeature aggregation module to integrate the extracted scene\ncontext features ( Fscene\ni ), enabling simultaneous genera-\ntion of multiple 3D assets. The module consists of a lo-\ncal attention block that refines details of individual as-\nsets, a global attention block that incorporates scene con-\ntext information and facilitates inter-asset interactions dur-\ning generation, and a feedforward network. Specifically,\nwe parameterize the local attention blocks and feedforward\nnetworks with pre-trained weights from TRELLIS [59], a\nflow-matching [36] model that sytnehsizes 3D content from\nnoisy sparse structure latents. For clarity and concise-\nness, given the sparse structure latents ( {xi}N\ni=1, where each\nxi\u2208RT\u00d7C) ofNobjects within a scene, we denote the\nstandard attention mechanism as Attention( Q,K,V), and\nelaborate on a single DiT block as follows.\nThelocal attention block aims to enhance details of in-\ndividual objects through asset-level self-attention (AS) and\ncross-attention (AC). Concretely, it focuses on fusing the\nlatent features of each object ( xi) with their corresponding\nvisual features ( FV\ni) to yield refined representations of each\nobject ( xAC\ni), which can be formulated as:\nxAS\ni= Attention( xi,xi,xi)\nxAC\ni= Attention( xAS\ni,FV\ni,FV\ni)\nTo establish inter-dependencies among 3D assets, we\npropose the global attention block, comprising scene-level\nself-attention (SS) and cross-attention (SC), which capture\ninter-object relationships and integrate scene geometry, re-\nspectively. This ensures physically plausible spatial ar-rangements of generated assets.\nSimilar to [53], we initialize one learnable position to-\nken (pi) and four register tokens ( ri) [7] for refined features\nof each object ( xAC\ni), denoted as: \u02c6xi= [pi;ri;xAC\ni], where\n[\u00b7;\u00b7]refers to concatenation along the token length dimen-\nsion. Notably, we assign a unique position token ( pquery )\nand register tokens ( rquery ) to the query asset, while adopt-\ning shared position token ( pi) and register tokens ( ri) for\nother assets. For each asset feature ( \u02c6xi\u2208RT\u00d7C), we con-\ncatenate them along the token sequence dimension to form\na unified scene representation ( X\u2208R(N\u00b7T)\u00d7C), which is\nprocessed by our scene-level self-attention layer, resulting\nin updated tokens of each asset ( {xSS\ni}N\ni=1), formulated as:\n{xSS\ni}N\ni=1= Attention( X,X,X)\nThrough this process, intra-asset and inter-asset informa-\ntion aggregation establishes essential shape and position\nawareness for coherent multi-asset generation. We then em-\nploy scene-level cross-attention to integrate multiple pre-\nextracted scene-aware features, thus incorporating 3D geo-\nmetric context. The features of each asset are updated into\ngeometry-aware representations ( {xSC\ni}N\ni=1), denoted as:\nxSC\ni= Attention( xSS\ni,Fscene\ni,Fscene\ni)\nThis preserves object-specific details while integrating\nglobal geometric constraints, which effectively addresses\nocclusion challenges and enables geometric refinement.\nOutput module. After passing through MDiT blocks,\nwe obtain the updated position tokens ( {\u02c6p}N\ni=1) and la-\ntent features ( {\u02dcx}N\ni=1) of each generated asset, which are\n4\n\n--- Page 5 ---\nthen decoded into their relative spatial positions ( {P}N\ni=1),\nand structural and texture representations ( {S}N\ni=1), respec-\ntively. For relative positions, we extract and concatenate\nthe position tokens of all non-query assets, which are then\ndecoded into corresponding 8D position vectors ( {\u02c6Pi}N\ni=2)\nby our proposed position head ( \u03a8pos), comprising four self-\nattention layers and a linear layer, denoted as:\n{\u02c6Pi}N\ni=2={[\u02c6ti,\u02c6qi,\u02c6si]}N\ni=2= \u03a8 pos({\u02c6pi}N\ni=2)\nHere, each spatial position vector ( \u02c6Pi) represents an as-\nset\u2019s spatial position (translation, rotation, and scale) rela-\ntive to the pre-selected query asset ( qquery ). Additionally,\nthe latent features can be directly decoded into the geom-\netry and texture of each asset ( {S}N\ni=1) using off-the-shelf\nsparse structure generator ( GS) and structured latents gen-\nerator ( GL) in TRELLIS [59], represented as:\n{S}N\ni=1=GL(GS({\u02dcx}N\ni=1))\n3.3. Training\nDuring training, only the global attention blocks, learnable\nposition tokens, and position head are optimized, with all\nother parameters frozen to facilitate efficient training, as de-\npicted in Figure 3. The technical details regarding training\ndata and loss function designs are presented below.\nTraining data. Our SceneGen model is trained on 3D-\nFUTURE [14], containing photorealistic scene renderings\nwith instance masks and asset annotations. This dataset\ncomprises 12K training scenes and 4.8K test scenes, each\nfeaturing a scene image with one or multiple objects. To\nbetter capture inter-object spatial relationships, we augment\nthe training set by iteratively designating each asset as the\nquery asset, and randomly permuting the remaining assets,\nwhich expands the effective training samples to 30K.\nTraining objectives. Our SceneGen model is trained end-\nto-end using a composite loss function ( L) comprising three\nkey components: (i) the average conditional flow match-\ning [36] loss ( Lcfm), on each generated asset for supervis-\ning asset generation; (ii) the position loss ( Lpos) for main-\ntaining accurate relative spatial arrangements among assets;\nand (iii) the voxel-space collision loss ( Lcoll) for enforcing\nphysically plausible object placements. The overall objec-\ntive function ( L) combines these components with a weight-\ning factor ( \u03bb), which can be formulated as:\nL=Lcfm+\u03bb(Lpos+Lcoll)\nConcretely, the flow matching loss establishes straight prob-\nability paths between distributions via linear interpolation:\nxi(t) = (1 \u2212t)x0\ni+t\u03f5, where \u03f5\u223c N(0,I),t\u2208[0,1], and\nx0\nidenotes the noise-free sparse structure latents for each\nof the Nassets in the scene. The conditional flow matchingobjective learns a parameterized function v\u03b8to approximate\nthe velocity field ( v(xi(t), t) =\u2207txi(t)), represented as:\nLcfm(\u03b8) =1\nNNX\ni=1Et,\u03f5\u2225v\u03b8(xi(t), t)\u2212(\u03f5\u2212x0\ni)\u22252\n2\nThe position loss ( Lpos) adopts a \u00b5-weighted Huber loss ( \u2225\u00b7\n\u2225\u03b4P) between the predicted positions ( \u02c6Pi= [\u02c6ti,\u02c6qi,\u02c6si])\nfor all non-query assets ( i\u2208[2, . . . , N ]) and their ground\ntruth (Pi= [ti,qi,si]), denoted as:\nLpos=NX\ni=2(\u00b5t\u2225(\u02c6ti\u2212ti)/dscene\u2225\u03b4P\n+\u00b5q\u2225\u02c6qi\u2212qi\u2225\u03b4P+\u00b5s\u2225\u02c6si\u2212si\u2225\u03b4P)\nHere, the translation error component is normalized by the\nscene scale ( dscene ) of each sample to mitigate numerical\ninstability caused by varying query asset selections. This\nstabilizes translation loss during training while improving\ngeneralization across distinct query asset configurations.\nThe collision loss ( Lcoll) quantifies surface collision in\na64\u00d764\u00d764voxel grid ( V). Concretely, the pre-\ndicted sparse structure latents ( \u02dcxi) are decoded into point\nclouds ( {pi}L\ni=1) via a pretrained sparse structure decoder\nfrom TRELLIS [59], then transformed using predicted pose\nparameters ( \u02c6Pi) and voxelized into V. The collision loss\nis defined as the ratio of overlapping surface voxels to all\nsurface voxels, using the Huber loss ( \u2225 \u00b7 \u2225\u03b4C), denoted as:\nLcoll=\u2225IoUscene\u2225\u03b4C=\u2225P\niI[Vi>1]P\niI[Vi>0]\u2225\u03b4C\nHere, IoUscene = 0means there are no asset collisions.\n3.4. Extension to Multi-view Inputs\nDespite only trained exclusively on single-image samples,\nour model surprisingly demonstrates inherent multi-view\ncompatibility through its flexible feature extraction and con-\nditioning strategy. Specifically, for a scene with Kinput\nviews ( {Ik\nScene}K\nk=1), each view\u2019s visual features ( Fk\nV) are\nindependently extracted via the visual encoder ( \u03a6V), while\nthe geometric features are obtained from the unified scene\nrepresentations encoded by aggregating information across\nall views with the geometric encoder ( \u03a6G), denoted as:\nFk\ngeo= \u03a6 G({Ij\nScene}K\nj=1)[k]\nThe final asset positions are determined by averaging pre-\ndictions across all views. Experiments show that this multi-\nview inference scheme improves asset generation quality\nthrough better geometric understanding, although the model\nhas never been explicitly fine-tuned on such inputs.\n5\n\n--- Page 6 ---\nMethodInstance\nSpecificGeometric Metrics Image\nCategoryVisual Metrics Inference\nTime (s) CD-S\u2193CD-O\u2193F-Score-S \u2191F-Score-O \u2191IoU-B\u2191 PSNR\u2191SSIM\u2191LPIPS \u2193FID\u2193CLIP-S \u2191DINO-S \u2191\nPartCrafter [34] % 0.2027 \u2014 40.43 \u2014 \u2014Scene\nGT-Render\u2014 \u2014 \u2014 \u2014 \u2014 \u20147.2\u2014 \u2014 \u2014 \u2014 \u2014 \u2014\nDepR [72] ! 0.0518 0.0862 63.02 47.66 0.2989Scene\nGT-Render\u2014 \u2014 \u2014 \u2014 \u2014 \u201411.6\u2014 \u2014 \u2014 \u2014 \u2014 \u2014\nGen3DSR [10] ! 0.0521 0.0935 61.26 41.26 0.2978Scene\nGT-Render15.92 0.8885 0.1730 63.95 0.8059 0.4334179.015.43 0.8899 0.1660 78.26 0.7950 0.4416\nMIDI\u2217[23] ! 0.0501 0.0602 68.74 61.04 0.2493Scene\nGT-Render16.93 0.8814 0.1778 22.75 0.8711 0.689242.515.45 0.8814 0.1711 28.26 0.8706 0.7034\nSceneGen ! 0.0118 0.0138 90.60 89.73 0.5818Scene\nGT-Render16.76 0.8903 0.1417 19.59 0.9152 0.832226.017.59 0.8991 0.1234 12.34 0.9236 0.8702\nTable 1. Quantitative Comparisons on the 3D-FUTURE Test Set. We evaluate the geometric structure using scene-level Chamfer\nDistance (CD-S) and F-Score (F-Score-S), object-level Chamfer Distance (CD-O) and F-Score (F-Score-O), and volumetric IoU of object\nbounding boxes (IoU-B). For visual quality, CLIP-S and DINO-S represent CLIP and DINOv2 image-to-image similarity, respectively. We\nreport the time cost for generating a single asset on a single A100 GPU, and\u2217indicates adopting MV-Adapter [24] for texture rendering.\n4. Experiments\nThis section starts with the experimental settings in Sec. 4.1,\nfollowed by comprehensive quantitative and qualitative\nevaluations in Sec. 4.2 and Sec. 4.3, respectively. Finally,\nwe conduct ablation studies in Sec. 4.4 to validate the effec-\ntiveness of our proposed model and strategies.\n4.1. Experimental Settings\nImplementation details. All experiments are conducted on\n8\u00d7NVIDIA A100 GPUS, where we train SceneGen for\n240 epochs using the AdamW [39] optimizer with a learn-\ning rate of 5\u00d710\u22125and a batch size of 8. The weight-\ning factor \u03bbdecays dynamically within [0.2,1]using a de-\ncay factor of 0.99, and the thresholds of Huber loss \u03b4Pand\n\u03b4Care set to 0.02 and 0.05, respectively. To handle vary-\ning numbers of assets across training scenes, each training\nstep dynamically samples scenes containing identical asset\ncounts. During inference, we adopt 25 sampling steps with\nthe classifier-free guidance (CFG) weight set to w= 5.0.\nEvaluation metrics. We assess the generated 3D scenes\nfrom both geometric and visual perspectives. For geome-\ntry, we reconstruct point clouds from the synthesized asset\nsurfaces and align them with the ground truth using Fil-\nterReg [17] for faster and more accurate registration than\ntraditional Iterative Closest Point (ICP [2]). We then com-\npute commonly used point cloud metrics, Chamfer Dis-\ntance (CD) and F-Score, at both scene and object levels,\nas well as the volumetric IoU of asset bounding boxes.\nFor visual quality, we focus on the scene texture render-\ning. Concretely, after alignment with the ground truth point\ncloud, we render the predicted scenes with Blender from the\noriginal input camera viewpoint. We consider two types of\nground truth: (i) instance-masked scene images extracted\nusing corresponding object masks, and (ii) images rendered\nfrom ground truth assets at the same viewpoint (excluding\nambient lighting). We compare our rendered results withboth types of ground truth using PSNR, SSIM, LPIPS [70],\nFID [21], CLIP [44] similarity, and DINOv2 [42] similarity\nto assess the texture quality of generated assets. Regarding\nefficiency, we report the inference time cost for synthesiz-\ning a single 3D asset on a single A100 GPU. More details\nwill be included in Sec. C.2 of the Appendix .\nBaselines. We compare SceneGen with representative\n3D scene generation methods, including PartCrafter [34],\nDepR [72], Gen3DSR [10], and MIDI [23], using their\npre-trained models. Specifically, we adopt object masks\nto specify generation targets for all baselines except for\nPartCrafter, which does not support mask-based control. In-\nstead, we directly provide PartCrater with extracted objects\nand the number of assets as input. Moreover, as PartCrafter\nand DepR do not offer code for texture rendering, our eval-\nuation of these methods focuses on geometric quality, while\nvisual quality is compared with Gen3DSR and MIDI (rely-\ning on MV-Adapter [24] for texture synthesis).\nBenchmarks. All evaluations are conducted on the 3D-\nFUTURE [14] test set, comprising 4.8K scenes. Each scene\ncontains a photorealistic rendered image with one or more\nobjects and corresponding segmentation masks as input.\n4.2. Quantitative Results\nAs presented in Table 1, we draw the following key obser-\nvations: (i) geometric quality : SceneGen consistently out-\nperforms existing methods across all scene-level and asset-\nlevel metrics. This stems from its joint integration of lo-\ncal asset features and global scene context during genera-\ntion. The interactions among multiple assets facilitate the\nmodel to produce physically plausible geometric structures,\nwhile the position head further improves the structural re-\nalism by explicitly predicting spatial arrangements. (ii) vi-\nsual quality : SceneGen can render high-quality textures for\ngenerated 3D assets without relying on any external tex-\nture generation models. Moreover, whether using masked\nscene images or ground-truth renderings as references, our\n6\n\n--- Page 7 ---\nScene ImageGT AssetsDepRGen3DSRPartCrafterMIDISceneGen(Ours)NotAvailableFigure 4. Qualitative Comparisons on the 3D FUTURE Test Set and ScanNet++. Our proposed SceneGen is capable of generating\nphysically plausible 3D scenes featuring complete structures ,detailed textures , and precise spatial relationships , demonstrating supe-\nrior performance over prior methods in terms of both geometric accuracy and visual quality on both the synthetic and real-world datasets.\nmethod consistently achieves the best performance across\nall metrics. This indicates that our synthesized assets are\nspatially closer to the ground truth while maintaining supe-\nrior texture fidelity. and (iii) efficiency : While PartCrafter\ndemonstrates a clear advantage in inference speed, it suffers\nfrom limited generation quality and controllability. In con-\ntrast, SceneGen achieves both superior quality and a strong\nbalance between quality and efficiency, synthesizing a 3D\nscene containing four assets with geometry and textures\nwithin 2 minutes on a single A100 GPU.\nIn addition, while the baseline methods, e.g., PartCrafter,\nDepR, and MIDI have been trained on 3D-FRONT [13],\nwhich may overlap with our test data, SceneGen still consis-\ntently outperforms them across all metrics, further demon-\nstrating its effectiveness and superiority.\n4.3. Qualitative Results\nComparisons with baselines. As depicted in Figure 4,\nwe qualitatively compare SceneGen with existing base-lines on both the 3D FUTURE test set and in-the-wild Sc-\nnaNet++ [63], where they still struggle with 3D scene gen-\neration: PartCrafter lacks controllability over the generated\ntargets and often mistakenly merges distinct assets, while\nboth PartCrafter and DepR are limited to geometry gener-\nation and cannot render textures. More critically, all these\nmethods exhibit difficulties in accurately understanding the\nspatial relationships among assets. In contrast, our pro-\nposed SceneGen precisely predicts the spatial relationships\namong assets and generates multiple 3D assets with accu-\nrate geometry and high-quality textures, without relying on\nany additional tools. These results clearly demonstrate the\neffectiveness and superiority of our approach.\nExtension to multi-image inputs. Benefiting from our ar-\nchitecture design, SceneGen can seamlessly handle multi-\nimage inputs after being trained solely on single-image\nsamples. Given the absence of suitable datasets for quan-\ntitative evaluation, we qualitatively assess the impact of\nmulti-image inputs by randomly sampling several scenes\n7\n\n--- Page 8 ---\nFgeo\nglobal FV\nglobal Fmask\niASSGeometric Metrics Image\nCategoryVisual Metrics\nCD-S\u2193CD-O\u2193F-Score-S \u2191F-Score-O \u2191IoU-B\u2191 PSNR\u2191SSIM\u2191LPIPS \u2193FID\u2193CLIP-S \u2191DINO-S \u2191\n! ! ! ! 0.0118 0.0138 90.60 89.73 0.5818Scene\nGT-Render16.76 0.8903 0.1417 19.59 0.9152 0.8322\n17.59 0.8991 0.1234 12.34 0.9236 0.8702\n% ! ! ! 0.0183 0.0266 83.33 74.71 0.4805Scene\nGT-Render15.89 0.8845 0.1574 20.21 0.9049 0.8063\n16.27 0.8918 0.1421 15.36 0.9125 0.8420\n% % ! ! 0.0250 0.0286 79.08 73.46 0.4253Scene\nGT-Render15.56 0.8806 0.1655 20.68 0.8980 0.7850\n15.86 0.8873 0.1511 16.62 0.9046 0.8187\n% % % ! 0.0310 0.0290 75.20 73.17 0.3825Scene\nGT-Render15.30 0.8773 0.1730 21.12 0.8932 0.7737\n15.55 0.8837 0.1591 17.45 0.9000 0.8076\n% % % % 0.0764 0.0352 54.21 70.55 0.1705Scene\nGT-Render13.32 0.8418 0.2329 27.56 0.8399 0.6059\n13.39 0.8464 0.2217 28.61 0.8440 0.6362\nTable 2. Ablations on SceneGen Variants. We progressively remove global geometric features ( Fgeo\nglobal ), global visual features ( FV\nglobal ),\nmask visual features ( Fmask\ni ), and substitute the scene-level self-attention ( ASS) to validate each component\u2019s contribution to SceneGen.\nSingle Image Input\nMulti-Image Input\nFigure 5. Qualitative Results with Multi-view Inputs. Scene-\nGen can directly handle multi-view inputs in ScanNet++ and even\nachieves better generation quality, especially accurate structure.\nfrom ScanNet++ [63] and employing SAM2 [46] to obtain\nsegmentation masks of corresponding objects. As depicted\nin Figure 5, compared to single-image inputs, incorporating\nmulti-view images leads to 3D assets with more complete\ngeometry and finer texture details. This illustrates that Sce-\nneGen can adaptively integrate complementary information\nfrom multiple views to produce higher-quality 3D scenes,\nfurther validating its practicality and scalability. More qual-\nitative results will be included in Sec C.1 of the Appendix .\n4.4. Ablation Studies\nTo validate the effectiveness of our modules, we conduct\ncomprehensive evaluations on several variants of Scene-Gen, assessing both geometric and visual quality of synthe-\nsized scenes. Concretely, we gradually remove global ge-\nometric features ( Fgeo\nglobal), global visual features ( FV\nglobal ),\nmask visual features ( Fmask\ni ), and substitute the scene-level\nself-attention block ( ASS) with a simple asset-level self-\nattention block ( AAS) to evaluate their impact on SceneGen.\nAs depicted in Table 2, we have the following observations:\n(i) Removing any aforementioned components degrades the\noverall performance, confirming their importance in Scene-\nGen; (ii) The geometric features primarily affect the struc-\nture of synthesized scenes, while the visual features further\nimpact the visual quality; and (iii) The absence of scene-\nlevel self-attention blocks eliminates inter-asset interactions\nduring generation, leading to notable performance declines\nacross all metrics. These results strongly demonstrate the\nnecessity and effectiveness of our proposed feature extrac-\ntion and aggregation modules for SceneGen.\n5. Conclusion\nIn this paper, we propose SceneGen , which takes a single\nscene image and target asset masks as input, simultaneously\nsynthesizing multiple 3D assets with structure and texture,\nas well as relative spatial positions in one feedforward pass.\nSpecifically, we incorporate dedicated visual and geometric\nencoders to extract both asset-level and scene-level features,\nwhich are effectively fused with our introduced feature ag-\ngregation module. Notably, through our meticulous design,\nSceneGen can even directly generalize to multi-image in-\nputs and achieve even better generation quality. Quantita-\ntive and qualitative evaluations demonstrate that SceneGen\ncan generate physically plausible and mutually consistent\n3D assets, significantly outperforming previous methods in\nterms of generation quality and efficiency.\nAcknowledgments\nWeidi would like to acknowledge the funding from Sci-\nentific Research Innovation Capability Support Project for\nYoung Faculty (ZY-GXQNJSKYCXNLZCXM-I22).\n8\n\n--- Page 9 ---\nReferences\n[1] Titas Anciukevi \u02c7cius, Zexiang Xu, Matthew Fisher, Paul Hen-\nderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-\nderdiffusion: Image diffusion for 3d reconstruction, inpaint-\ning and generation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2023. 2\n[2] PJ Besl and Neil D McKay. A method for registration of 3-d\nshapes. IEEE Transactions on Pattern Analysis and Machine\nIntelligence , 14(2):239\u2013256, 1992. 6, 15\n[3] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and\nAnpei Chen. Easi3r: Estimating disentangled motion from\ndust3r without training. In Proceedings of the International\nConference on Computer Vision , 2025. 2\n[4] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-\nder G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal\n3d shape completion, reconstruction, and generation. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 4456\u20134465, 2023. 2\n[5] Tao Chu, Pan Zhang, Qiong Liu, and Jiaqi Wang. Buol:\nA bottom-up framework with occupancy-aware lifting for\npanoptic 3d scene reconstruction from a single image. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 4937\u20134946, 2023. 2, 3\n[6] Jasmine Collins, Shubham Goel, Kenan Deng, Achlesh-\nwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas\nF Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al.\nAbo: Dataset and benchmarks for real-world 3d object un-\nderstanding. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2022. 2\n[7] Timoth \u00b4ee Darcet, Maxime Oquab, Julien Mairal, and Piotr\nBojanowski. Vision transformers need registers. In Proceed-\nings of the International Conference on Learning Represen-\ntations , 2024. 4\n[8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,\nOscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,\nVikram V oleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A\nuniverse of 10m+ 3d objects. Conference on Neural Infor-\nmation Processing Systems , 36:35799\u201335813, 2023. 2\n[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2023. 2\n[10] Andreea Dogaru, Mert \u00a8Ozer, and Bernhard Egger. Gen3dsr:\nGeneralizable 3d scene reconstruction via divide and con-\nquer from a single view. In International Conference on 3D\nVision , 2025. 3, 6\n[11] Wenqi Dong, Bangbang Yang, Zesong Yang, Yuan Li, Tao\nHu, Hujun Bao, Yuewen Ma, and Zhaopeng Cui. Hiscene:\ncreating hierarchical 3d scenes with isometric view genera-\ntion. arXiv preprint arXiv:2504.13072 , 2025. 2, 3\n[12] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Ar-\njun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and\nWilliam Yang Wang. Layoutgpt: Compositional visual plan-\nning and generation with large language models. In Confer-ence on Neural Information Processing Systems , 2023. 2,\n3\n[13] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming\nWang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Bin-\nqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts\nand semantics. In Proceedings of the International Confer-\nence on Computer Vision , 2021. 2, 7\n[14] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang\nZhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d fur-\nniture shape with texture. International Journal of Computer\nVision , 129:3313\u20133337, 2021. 2, 5, 6, 13\n[15] Daoyi Gao, D \u00b4avid Rozenberszki, Stefan Leutenegger, and\nAngela Dai. Diffcad: Weakly-supervised probabilistic cad\nmodel retrieval and alignment from an rgb image. ACM\nTransactions On Graphics , 43(4):1\u201315, 2024. 2, 3\n[16] Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and\nBernhard Sch \u00a8olkopf. Graphdreamer: Compositional 3d\nscene synthesis from scene graphs. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 21295\u201321304, 2024. 2, 3\n[17] Wei Gao and Russ Tedrake. Filterreg: Robust and efficient\nprobabilistic point-set registration using gaussian filter and\ntwist parameterization. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , 2019. 6,\n15\n[18] Zeqi Gu, Yin Cui, Zhaoshuo Li, Fangyin Wei, Yunhao\nGe, Jinwei Gu, Ming-Yu Liu, Abe Davis, and Yifan Ding.\nArtiscene: Language-driven artistic 3d scene generation\nthrough image intermediary. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n2025. 2, 3\n[19] Can G \u00a8umeli, Angela Dai, and Matthias Nie\u00dfner. Roca: Ro-\nbust cad model retrieval and alignment from a single image.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 4022\u20134031, 2022. 2, 3\n[20] Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yang-\nguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and\nTong He. Gvgen: Text-to-3d generation with volumetric rep-\nresentation. In Proceedings of the European Conference on\nComputer Vision , pages 463\u2013479, 2024. 2\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Conference on Neural Information Processing Sys-\ntems, 2017. 6\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Conference on Neural Infor-\nmation Processing Systems , 2020. 2\n[23] Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan\nYang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu,\nYan-Pei Cao, and Lu Sheng. Midi: Multi-instance diffu-\nsion for single image to 3d scene generation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 23646\u201323657, 2025. 2, 3, 6, 14, 15\n[24] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi,\nLizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter:\nMulti-view consistent image generation made easy. In Pro-\n9\n\n--- Page 10 ---\nceedings of the International Conference on Computer Vi-\nsion, 2025. 2, 6\n[25] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural\nwavelet-domain diffusion for 3d shape generation. In ACM\nSIGGRAPH Asia Conference , pages 1\u20139, 2022. 2\n[26] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng,\nXin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu,\nYunfei Zhao, et al. Hunyuan3d 2.1: From images to high-\nfidelity 3d assets with production-ready pbr material. arXiv\npreprint arXiv:2506.15442 , 2025. 2\n[27] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and\nAndrea Vedaldi. Geo4d: Leveraging video generators for\ngeometric 4d scene reconstruction. In Proceedings of the\nInternational Conference on Computer Vision , 2025. 2\n[28] Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay\nHaresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg,\nEric Undersander, Angel X Chang, and Manolis Savva.\nHabitat synthetic scenes dataset (hssd-200): An analysis of\n3d scene scale and realism tradeoffs for objectgoal naviga-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2024. 2\n[29] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. In Proceedings of the International Conference\non Learning Representations , 2014. 2\n[30] Vincent Leroy, Yohann Cabon, and J \u00b4er\u02c6ome Revaud. Ground-\ning image matching in 3d with mast3r. In Proceedings of the\nEuropean Conference on Computer Vision , 2024. 2\n[31] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with\nsparse-view generation and large reconstruction model. In\nProceedings of the International Conference on Learning\nRepresentations , 2024. 2\n[32] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-\nsdf: Text-to-shape via voxelized diffusion. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 12642\u201312651, 2023. 2\n[33] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan\nLiang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding\nLiang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape\nsynthesis using large-scale rectified flow models. arXiv\npreprint arXiv:2502.06608 , 2025. 2\n[34] Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan,\nYiqiang Feng, Yadong Mu, and Katerina Fragkiadaki.\nPartcrafter: Structured 3d mesh generation via compo-\nsitional latent diffusion transformers. arXiv preprint\narXiv:2506.05573 , 2025. 2, 3, 6\n[35] Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu\nZeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera,\nand Zhaoshuo Li. Scenethesis: A language and vision\nagentic framework for 3d scene generation. arXiv preprint\narXiv:2505.02836 , 2025. 2, 3\n[36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matthew Le. Flow matching for generative\nmodeling. In Proceedings of the International Conference\non Learning Representations , 2023. 4, 5, 13\n[37] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yan-\nfeng Wang, and Weidi Xie. Intelligent grimm - open-endedvisual storytelling via latent diffusion models. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition , 2024. 2\n[38] Xinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. Agentic 3d\nscene generation with spatially contextualized vlms. arXiv\npreprint arXiv:2505.20129 , 2025. 2, 3\n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In Proceedings of the International Confer-\nence on Learning Representations , 2019. 6\n[40] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition , 2021.\n2\n[41] Norman M \u00a8uller, Yawar Siddiqui, Lorenzo Porzi,\nSamuel Rota Bulo, Peter Kontschieder, and Matthias\nNie\u00dfner. Diffrf: Rendering-guided 3d radiance field diffu-\nsion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pages 4328\u20134338, 2023. 2\n[42] Maxime Oquab, Timoth \u00b4ee Darcet, Th \u00b4eo Moutakanni, Huy\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\nTransactions on Machine Learning Research , 2024. 2, 3, 6,\n13\n[43] William Peebles and Saining Xie. Scalable diffusion mod-\nels with transformers. In Proceedings of the International\nConference on Computer Vision , 2023. 2, 3\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In Proceedings of the International Conference on\nMachine Learning , 2021. 6\n[45] Ren \u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 12179\u201312188, 2021. 13\n[46] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR\u00a8adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Doll \u00b4ar, and Christoph Feicht-\nenhofer. Sam 2: Segment anything in images and videos.\nInProceedings of the International Conference on Learning\nRepresentations , 2025. 8\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bjorn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2022. 2\n[48] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , 2016. 2\n[49] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam\nBhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun\nWu. Layoutvlm: Differentiable optimization of 3d layout via\nvision-language models. In Proceedings of the IEEE Confer-\n10\n\n--- Page 11 ---\nence on Computer Vision and Pattern Recognition , 2025. 2,\n3\n[50] Xiang Tang, Ruotong Li, and Xiaopeng Fan. Towards geo-\nmetric and textural consistency 3d scene generation via sin-\ngle image-guided model generation and layout optimization.\narXiv preprint arXiv:2507.14841 , 2025. 3\n[51] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wen-\nzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua\nShen, Jiangmiao Pang, et al. Aether: Geometric-aware uni-\nfied world modeling. In Proceedings of the International\nConference on Computer Vision , 2025. 2\n[52] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and\nDavid Novotny. Vggsfm: Visual geometry grounded deep\nstructure from motion. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , 2024. 2\n[53] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea\nVedaldi, Christian Rupprecht, and David Novotny. Vggt: Vi-\nsual geometry grounded transformer. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2025. 2, 3, 4, 13, 14\n[54] Qianqian Wang, Yifei Zhang, Aleksander Holynski,\nAlexei A Efros, and Angjoo Kanazawa. Continuous 3d per-\nception model with persistent state. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2025.\n[55] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris\nChidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vi-\nsion made easy. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2024. 2\n[56] Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang,\nYa Zhang, and Yanfeng Wang. Megafusion: Extend dif-\nfusion models towards higher-resolution image generation\nwithout further tuning. In Winter Conference on Applica-\ntions of Computer Vision , 2025. 2\n[57] Haoning Wu, Ziheng Zhao, Ya Zhang, Weidi Xie, and Yan-\nfeng Wang. Mrgen: Diffusion-based controllable data en-\ngine for mri segmentation towards unannotated modalities.\nInProceedings of the International Conference on Computer\nVision , 2025. 2\n[58] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi\nXu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable\nimage-to-3d generation via 3d latent diffusion transformer.\nInConference on Neural Information Processing Systems ,\n2024. 2\n[59] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng\nWang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong\nYang. Structured 3d latents for scalable and versatile 3d gen-\neration. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2025. 2, 3, 4, 5, 13\n[60] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Ji-\nahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein,\nZexiang Xu, et al. Dmv3d: Denoising multi-view diffu-\nsion using 3d large reconstruction model. In Proceedings of\nthe International Conference on Learning Representations ,\n2024. 2\n[61] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro\nHerrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Kr-ishna, Lingjie Liu, et al. Holodeck: Language guided gener-\nation of 3d embodied ai environments. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2024. 2, 3\n[62] Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qix-\nuan Zhang, Wei Yang, Lan Xu, Jiayuan Gu, and Jingyi Yu.\nCast: Component-aligned 3d scene reconstruction from an\nrgb image. In ACM SIGGRAPH Conference , 2025. 2, 3\n[63] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner,\nand Angela Dai. Scannet++: A high-fidelity dataset of 3d in-\ndoor scenes. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 12\u201322, 2023. 7, 8, 14\n[64] Huangyue Yu, Baoxiong Jia, Yixin Chen, Yandan Yang,\nPuhao Li, Rongpeng Su, Jiaxin Li, Qing Li, Wei Liang,\nSong-Chun Zhu, et al. Metascenes: Towards automated\nreplica creation for real-world 3d scans. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 1667\u20131679, 2025. 2, 3\n[65] Guangyao Zhai, Evin P\u0131nar \u00a8Ornek, Shun-Cheng Wu, Yan\nDi, Federico Tombari, Nassir Navab, and Benjamin Busam.\nCommonscenes: Generating commonsense 3d indoor scenes\nwith scene graph diffusion. In Conference on Neural Infor-\nmation Processing Systems , 2023. 2, 3\n[66] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3dshape2vecset: A 3d shape representation for neu-\nral fields and generative diffusion models. ACM Transactions\nOn Graphics , 42(4):1\u201316, 2023. 2\n[67] Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang,\nFeng Zhao, Yansong Tang, Dong Chen, and Baining Guo.\nGaussiancube: A structured and explicit radiance represen-\ntation for 3d generative modeling. In Conference on Neural\nInformation Processing Systems , 2024. 2\n[68] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jam-\npani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-\nHsuan Yang. Monst3r: A simple approach for estimating ge-\nometry in the presence of motion. In Proceedings of the In-\nternational Conference on Learning Representations , 2025.\n2\n[69] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu,\nAnqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu.\nClay: A controllable large-scale generative model for creat-\ning high-quality 3d assets. ACM Transactions On Graphics ,\n43(4):1\u201320, 2024. 2\n[70] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2018. 6\n[71] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue,\nChristian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gor-\ndon Wetzstein. Flare: Feed-forward geometry, appearance\nand camera estimation from uncalibrated sparse views. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , 2025. 2\n[72] Qingcheng Zhao, Xiang Zhang, Haiyang Xu, Zeyuan Chen,\nJianwen Xie, Yuan Gao, and Zhuowen Tu. Depr: Depth\nguided single-view scene reconstruction with instance-level\ndiffusion. arXiv preprint arXiv:2507.22825 , 2025. 6\n11\n\n--- Page 12 ---\nSceneGen: Single-Image 3D Scene Generation in One Feedforward Pass\nAppendix\nContents\n1. Introduction 2\n2. Related Work 2\n3. Method 3\n3.1. Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n3.2. SceneGen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n3.3. Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3.4. Extension to Multi-view Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n4. Experiments 6\n4.1. Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4.2. Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4.3. Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n4.4. Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n5. Conclusion 8\nA . Preliminaries on 3D Foundation Models 13\nB . More Details about Training Data 13\nC . More Implementation Details 13\nC.1. Extension to Multi-image Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nC.2. Evaluation Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nD . More Visualizations 16\nE . Limitations & Future Works 17\nE.1. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nE.2. Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n12\n\n--- Page 13 ---\nA. Preliminaries on 3D Foundation Models\nGiven the inherent challenges of directly generating a 3D scene with multiple 3D assets from a single image, SceneGen aims\nto fully leverage the visual and geometric priors embedded in state-of-the-art 3D foundation models. Therefore, we build our\nmodel based on TRELLIS [59], and adopt DINOv2 [42] and VGGT [53] as our visual and geometric encoders, respectively.\nIn the following, we provide a detailed introduction to TRELLIS and VGGT to better illustrate their roles.\nTRELLIS. For a 3D asset ( O), TRELLIS encodes its geometry and appearance into a unified representation ( z), denoted as:\nz={(zi,pi)}L\ni=1. Here, pi\u2208 {0,1, . . . , D \u22121}3denotes the positional index of an active voxel intersecting the surface of\nO, andzi\u2208RCrepresents the corresponding local latent feature, with DandLrepresenting the 3D grid resolution and the\ntotal number of active voxels, respectively.\nThe generation process adopts two cascaded rectified flow models: the sparse structure generator (GS) synthesizes\nthe sparse voxel structure {pi}L\ni=1, encoding geometric priors by predicting its low-resolution feature gird ( S); while the\nstructured latents generator (GL) generates texture and appearance features {zi}L\ni=1conditioned on {pi}. Both models\nare optimized via the conditional flow matching (CFM) [36] objective, which establishes straight probability paths between\ndistributions through linear interpolation: x(t) = (1 \u2212t)x0+t\u03f5, where x0denotes data samples, \u03f5\u223c N(0,I), andt\u2208[0,1].\nThe velocity field ( v(x, t) =\u2207tx) governs the reverse process, with the CFM objective formulated as:\nLcfm(\u03b8) =Et,x0,\u03f5\u2225v\u03b8(x, t)\u2212(\u03f5\u2212x0)\u22252\n2\nNotably, the sparse structured generator ( GS) learns rich geometric priors from large-scale 3D data, effectively capturing\nboth object geometries and spatial relationships, thus delivering essential asset-level understanding capabilities. Our Scene-\nGen is compatible with both the sparse structured generator and structured latents generator, thus can sequentially employ\nthem to decode synthesized latent features into the geometry and texture of 3D assets.\nVGGT. Trained on large-scale 3D annotated data, VGGT can extract 3D scene features through a purely feedforward network\nwithout explicit 3D inductive biases. For single or multi-view RGB inputs ( {Ii}s\ni=1), its aggregator derives scene geometric\nfeatures ( {Fgeo\ni}s\ni=1) via:\n{Fgeo\ni}s\ni=1={[Fgeo\nG,Fgeo\nI]}s\ni=1= VGGT( {Ii}s\ni=1)\nHere,Fgeo\nGandFgeo\nIrepresent features extracted by global self-attention andself-attention layers, respectively. These\nfeatures are efficiently decoded by lightweight DPT layers [45] into depth maps, point maps, and tracks, validating their rich\nscene geometric representation capacity.\nBy integrating these complementary strengths, our SceneGen effectively captures both local asset-level and global scene-\nlevel features from the input image, achieving robust performance on the challenging 3D scene generation task.\nB. More Details about Training Data\nWe train our SceneGen model on the 3D-FUTURE [14] dataset, whose rich textures and diverse scene lighting conditions\neffectively simulate real-world environments and thus enhance the model\u2019s generalization ability. In addition, we further\nscale up training data through data augmentation to ensure the model can robustly learn the relative spatial relationships\namong multiple assets. Concretely, for a scene with Nobjects, during training, we iteratively select each asset as the query\nasset and randomly shuffle the remaining assets. Considering GPU memory constraints, we set the maximum number of\nassets per scene to N\u2032= 7 on a single A100 GPU. For samples containing more than N\u2032assets, we randomly select N\u2032\nassets from them for training. Furthermore, following TRELLIS [59], we also leverage its aesthetic score filtering criterion\nto filter out assets with aesthetic scores below 4.5, ensuring the high quality of training data. The distribution of asset counts\nacross training scenes is illustrated in Figure 6.\nC. More Implementation Details\nIn this section, we provide a comprehensive explanation of the implementation details discussed in the paper. Specifically,\nSec. C.1 describes the specific strategies applied to extend SceneGen to multi-image inputs; and Sec. C.2 elaborates on the\ndetails of our evaluation protocols.\nC.1. Extension to Multi-image Inputs\nWhile our model is designed for 3D scene generation from a single scene image and is trained only on datasets containing\nsingle-view images, SceneGen can be easily adapted to multi-view inputs by simply modifying the sampling process, without\n13\n\n--- Page 14 ---\nFigure 6. Distribution of Asset Counts in our Training Data and Original 3D-FUTURE.\nrequiring additional training or fine-tuning. Concretely, during inference, our model can take images of the same scene from\ndifferent viewpoints, along with their corresponding objects and instance masks, as input. The geometric encoder ( \u03a6G)\nwithin our SceneGen, specifically, an off-the-shelf VGGT [53] aggregator, integrates geometric information from different\nviewpoints to obtain better geometric representations for each perspective. This enables SceneGen to synthesize better\ngeometric structures during the generation process. Finally, we predict the relative positions among different assets from\neach viewpoint and use the mean of these predictions across all views as the final spatial position output. It is important to\nnote that, to ensure correctness throughout the inference process, the input order of assets and their segmentation masks must\nremain consistent across different viewpoints.\nGiven the current lack of training and quantitative evaluation data for multi-view 3D scene generation, this work presents\nqualitative results on scenes sampled from ScanNet++ [63] to demonstrate the scalability of SceneGen, and leaves the con-\nstruction of suitable multi-view datasets and evaluation methods for future work.\nMethod Alignment CD-S\u2193CD-S 1 \u2193CD-S 2 \u2193F-Score-S \u2191IoU-B\u2191\nMIDI [23]ICP 0.1697 0.0653 0.1044 41.64 0.1232\nFilterReg 0.0501 0.0278 0.0223 68.74 0.2493\nSceneGen (Ours)ICP 0.0310 0.0121 0.0189 83.74 0.5103\nFilterReg 0.0118 0.0052 0.0066 90.60 0.5818\nTable 3. Geometric Metric Comparisons with Different Point Cloud Alignment Methods.\nC.2. Evaluation Protocols\nGeometric metrics. Following previous work [23], we conduct geometry evaluation in normalized 3D space (also referred\nto as canonical space, i.e.,x, y, z \u2208[\u22121,1]), where the ground truth and the synthesized query asset are first rigidly aligned\n14\n\n--- Page 15 ---\nScene ImageInstance-masked ImageGT RenderPred RenderFigure 7. Examples of Visual Metrics Evaluation Protocols. Here, we present two complementary types of ground truth: instance-\nmasked images may introduce slight differences due to potential occlusions, while GT-render images lack scene-level illumination.\nusing point cloud registration algorithms. Unlike MIDI [23], which adopts the traditional Iterative Closest Point (ICP [2])\nmethod that produces suboptimal alignment results, we employ FilterReg [17], a fast, accurate, and robust point cloud\nalignment approach. As presented in Table 3, both MIDI and SceneGen achieve better overall performance when aligned\nvia FilterReg, demonstrating the reliability of this alignment method compared to traditional ICP. Moreover, under both\nalignment strategies, SceneGen consistently outperforms MIDI, indicating that explicitly predicting the spatial positions\namong assets enables SceneGen to more accurately model the relationships among distinct 3D assets within the scene.\nVisual metrics. Beyond the commonly used geometric evaluations described above, we also consider several visual metrics\n15\n\n--- Page 16 ---\nScene ImageGT AssetsDepRGen3DSRPartCrafterMIDISceneGen(Ours)Figure 8. More Qualitative Comparisons on the 3D FUTURE Test Set.\nto assess the visual quality of generated scenes. Concretely, after aligning the synthesized point clouds with the ground truth\nscenes, we use Blender to render them with the identical camera parameters. The rendered images are then compared with\ntwo types of ground truth to compute perceptual metrics that reflect the visual quality of synthesized scenes. As illustrated in\nFigure 7, these include: (i) instance-masked scene images, which are extracted using the corresponding object masks, where\nthe occlusion relationships between assets introduce differences relative to predicted renderings; and (ii) GT-Render images,\nwhich are rendered from the ground truth assets at the same viewpoint using Blender , but lack scene-level illumination\nand complete textures, resulting in textural discrepancies compared to predicted scenes. Thus, by computing visual metrics\nagainst both types of ground truth, we provide a complementary evaluation of the visual quality of synthesized scenes.\nEfficiency. To ensure a fair comparison across all methods, we report the average inference time over 500 trials of synthe-\nsizing scenes with a single asset on a single A100 GPU. Notably, our proposed SceneGen can directly generate 3D scenes\ncontaining 4 assets in a single feedforward pass within 2 minutes on the same hardware, eliminating the need for time-\nconsuming sequential generation of individual 3D assets.\nD. More Visualizations\nIn this section, we present additional visualization results on the 3D-FUTURE test set to qualitatively compare our SceneGen\nwith representative baselines. As depicted in Figure 8, we have the following observations: (i) PartCrafter frequently suffers\nfrom missing or mixed-up assets due to its inability to control generation via object masks, despite already taking segmented\nobjects and asset counts as input; (ii) Both PartCrafter and DepR can only generate scene geometry without rendering texture\ndetails; and (iii) All baseline methods (including Gen3DSR and MIDI) share the common limitation of incorrect spatial\nrelationships among synthesized assets. In contrast, our SceneGen fully integrates visual and geometric features within the\n16\n\n--- Page 17 ---\nscene to enable mutual influence among multiple assets during generation, producing 3D scenes with physically 3D scenes\nand high-quality texture details.\nE. Limitations & Future Works\nE.1. Limitations\nWhile our SceneGen demonstrates superior performance in 3D scene generation, it is not without its limitations. Specifically,\nalthough SceneGen demonstrates better texture generation and generalization capabilities compared to previous methods that\nrely on canonical representations, the narrow training data distribution limits its ability to generalize to non-indoor scenes,\nrestricting its generalization to a broader range of environments. Moreover, while SceneGen can generate multiple 3D assets\nand relative spatial positions in a single feedforward pass, without relying on complex post-processing, it does not always\nhandle contact relationships among objects, occasionally leading to asset overlaps or geometric inconsistencies. This is\nmainly because our single-stage framework does not explicitly enforce strict spatial or physical constraints among objects.\nE.2. Future Works\nTo address the aforementioned limitations of SceneGen, we propose several directions for future improvement: (i) Con-\nstructing larger-scale 3D scene generation datasets that cover more diverse indoor and outdoor scenarios, to address biases in\ntraining data distribution and improve the generalization ability of models; (ii) Building suitable multi-view scene generation\ndatasets to expand the application scope and practical potential of existing models; and (iii) Incorporating explicit physical\npriors or constraints to facilitate the model to better learn complex interactions among objects.\n17",
  "project_dir": "artifacts/projects/enhanced_cs.AI_2508.15769v1_SceneGen_Single_Image_3D_Scene_Generation_in_One_",
  "communication_dir": "artifacts/projects/enhanced_cs.AI_2508.15769v1_SceneGen_Single_Image_3D_Scene_Generation_in_One_/.agent_comm",
  "assigned_at": "2025-08-22T21:14:29.356063",
  "status": "assigned"
}